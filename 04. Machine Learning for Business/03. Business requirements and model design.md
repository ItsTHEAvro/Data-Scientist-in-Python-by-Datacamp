## Business Requirements

1.  **Identify the Business Situation:**
    * Clearly define the business problem or goal (e.g., expanding markets, increasing fraud, rising churn).
    * Start by asking **inference questions** (the "why"): Why is churn increasing? What indicates fraud?
    * Then, build to **prediction questions** (the "what will happen"): Can we identify at-risk customers? Can we flag risky transactions?

2.  **Assess the Opportunity Size:**
    * Estimate the potential improvement ML can bring (e.g., reduce churn by X%, prevent Y% of fraud costs).
    * Conduct a **cost/benefit analysis**: Compare the estimated value against the project cost.
    * Estimating opportunity size is challenging; **running experiments** with model predictions is the best approach.

3.  **Define Business Actions:**
    * Determine concrete actions based on ML outputs (e.g., prioritize investments, manual reviews, retention campaigns).
    * Ensure models are **actionable**; predictions need a plan for action.
    * **Run experiments** (e.g., targeting predicted groups) to confirm impact (lift or reduction).
    * Calculate potential ROI if actions are rolled out.
    * If experiments don't yield meaningful results, **revisit the business situation**: collect more data, conduct qualitative research, and refine the business question.

**Examples (Illustrative):**
* **Fraud Prediction:** Situation (increasing fraud), Opportunity (prevent costs/loss of trust), Action (improve detection, manual review).
* **Churn Prediction:** Situation (increasing churn), Opportunity (reduce churn, mitigate revenue impact), Action (identify/fix churn drivers, retention campaigns).

## Model Training
1.  **Dataset Preparation:**
    * Start with a full dataset containing input features and a target variable.

2.  **Data Splitting (Crucial Step):**
    * **Training Set:** A randomly sampled portion of the full dataset.
        * **Purpose:** Used by the machine learning model to learn patterns between input features and the target variable.
    * **Test Set (Unseen Data):** A smaller, separate portion of the dataset not used during training.
        * **Purpose:** To provide a realistic assessment of how the model will perform on new, future data. This helps avoid overly optimistic performance metrics from the training set.

3.  **Understanding Model Fit (Why Splitting Matters):**
    * **Underfitting:** The model is too simple and fails to capture the underlying patterns in the data (e.g., using a straight line for a curved relationship).
    * **Overfitting:** The model is too complex and "memorizes" the training data, including noise. It performs excellently on training data but poorly on unseen test data.
    * **Right Fit:** The model generalizes well from the training data and accurately predicts outcomes on unseen test data.

4.  **The Training Process:**
    * The machine learning model uses statistical algorithms on the **training dataset** to learn the relationship between features and the target variable (Y).

5.  **Performance Assessment:**
    * Measure the trained model's performance using the **unseen test dataset**.
    * **Importance:** Test set performance gives the best estimate of actual future model accuracy. Always verify with ML teams that reported results are from the test set.

6.  **Iteration & Selection:**
    * This process (train on training set, evaluate on test set) can be repeated with different models or configurations.
    * The goal is to find a model that fits the data best without underfitting or overfitting.

## Model Performance Measurement
Focuses on assessing the quality of supervised learning model predictions.

1.  **Two Main Types of Tasks & Metrics:**
    * **Classification:** Predicting categories/classes (e.g., churn status, fraud).
        * **Accuracy:** Overall correctness. (Total Correct Predictions) / (Total Observations).
            * *Example:* 9/10 correct classifications = 90% accuracy.
        * **Precision:** Of those predicted as positive (e.g., "churn"), how many actually were positive?
            * *Formula Idea:* (True Churners Predicted as Churn) / (All Predicted as Churn).
            * *Example:* Predicted 6 as churn, 5 actually churned. Precision = 5/6 â‰ˆ 83%.
            * *Focus:* Minimizing false positives (e.g., wrongly accusing a good customer of churn).
        * **Recall (Sensitivity):** Of all actual positive cases, how many did the model identify?
            * *Formula Idea:* (True Churners Predicted as Churn) / (All Actual Churners).
            * *Example:* Model identified all 5 actual churners out of 5. Recall = 5/5 = 100%.
            * *Focus:* Minimizing false negatives (e.g., failing to identify a customer who will churn).
        * **Precision-Recall Trade-off:** Improving one often worsens the other. Business context dictates which is more critical (e.g., is it worse to miss a churner or to misclassify a loyal customer as a churn risk?).

    * **Regression:** Predicting continuous values (e.g., sales, stock price, revenue).
        * **Error Metrics:** Key measure. Quantifies how far predictions are from the actual observed values.
            * *Example:* Summing up the differences (errors) between predicted revenue and actual revenue, then calculating an average error.
        * If error is too high, try different models (e.g., a non-linear curve if a straight line underperforms).

2.  **Beyond Metrics: Actionable Models & A/B Testing:**
    * Strong performance metrics are important, but not the sole determinant of a model's value.
    * **Models must be actionable:** Their predictions should lead to beneficial business actions.
    * **A/B Testing (Real-life Experiments):** Essential to validate a model's real-world impact.
        * *Example:* Target customers predicted as churners with incentives and compare their churn rate to a control group of predicted churners who received no incentive.
        * If ML-driven actions show improvements (e.g., reduced churn, increased purchases), the model is valuable and can be integrated into production systems.
        * If no improvements are observed, the model or the strategy needs re-evaluation ("back to the drawing board").

## Machine Learning Risks

1.  **Poor Performance Metrics (General):**
    * A primary risk if model predictions are not accurate enough to impact business outcomes.
    * **Crucial Check:** Always ensure performance metrics are based on the **test dataset**, not the training dataset, for a realistic assessment.

2.  **Specific Performance Risks:**
    * **Classification Models:**
        * **Low Precision:** Results in many false positives (e.g., predicting many customers will buy, but few actually do; predicting non-fraudulent transactions as fraud).
            * *Business Impact:* Assess the cost of acting on these incorrect positive predictions.
        * **Low Recall:** Model misses a significant portion of actual positive cases (false negatives) (e.g., failing to identify 75% of actual fraudulent transactions).
            * *Business Impact:* Requires model improvement or supplementary detection methods to capture missed cases.
    * **Regression Models:**
        * **Large Error:** Predictions are, on average, significantly different from actual values (e.g., a customer satisfaction prediction error of 3.5 on a 5-point scale, or a 70% average error).
            * *Business Impact:* Assess the cost of these mistakes and the business's tolerance for error levels.

3.  **Non-Actionable Model Use Cases:**
    * Even models with excellent performance metrics can be unusable if their outputs cannot be translated into effective business actions or don't improve outcomes.
    * **Testing for Actionability (A/B Testing):**
        * Once model performance is acceptable, conduct experiments (A/B tests).
        * *Example (Churn):* Predict customers likely to churn. Split them randomly: Group A (treatment) receives retention incentives; Group B (control) receives nothing.
        * Compare outcomes (e.g., churn rates) between Group A and Group B.
        * If there's a positive "lift" (e.g., Group A has lower churn), the model is likely actionable and a candidate for production.
        * If no lift, the model or the intervention strategy may not be effective.

4.  **Addressing Failed Tests or Improving Models:**
    * **Get More/Better Data:** Collaborate with ML teams; business expertise is vital to identify relevant and predictive data.
    * **Build Causal Models:** Understand the underlying drivers of the outcome (the "why").
    * **Qualitative Research & Surveys:** Gain insights into customer behavior and motivations.
    * **Change the Scope:**
        * Narrow the problem definition to be more specific.
        * Widen it to cover a broader question.
        * Change the business question altogether.
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5df45a",
   "metadata": {},
   "source": [
    "# Selecting relevant features\n",
    "\n",
    "In this exercise, you'll identify the redundant columns in the `volunteer` dataset, and perform feature selection on the dataset to return a DataFrame of the relevant features.\n",
    "\n",
    "For example, if you explore the `volunteer` dataset in the console, you'll see three features which are related to location: `locality` , `region` , and `postalcode` . They contain related information, so it would make sense to keep only one of the features.\n",
    "\n",
    "Take some time to examine the features of `volunteer` in the console, and try to identify the redundant features.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Create a list of redundant column names and store it in the `to_drop` variable: Out of all the location-related features, keep only `postalcode` . Features that have gone through the feature engineering process are redundant as well.\n",
    "- Out of all the location-related features, keep only `postalcode` .\n",
    "- Features that have gone through the feature engineering process are redundant as well.\n",
    "- Drop the columns in the `to_drop` list from the dataset.\n",
    "- Print out the `.head()` of `volunteer_subset` to see the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ac4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "volunteer = pd.read_csv(\"volunteer_opportunities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca4313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   opportunity_id  content_id  event_time  \\\n",
      "0            4996       37004           0   \n",
      "1            5008       37036           0   \n",
      "2            5016       37143           0   \n",
      "3            5022       37237           0   \n",
      "4            5055       37425           0   \n",
      "\n",
      "                                               title  hits  \\\n",
      "0  Volunteers Needed For Rise Up & Stay Put! Home...   737   \n",
      "1                                       Web designer    22   \n",
      "2      Urban Adventures - Ice Skating at Lasker Rink    62   \n",
      "3  Fight global hunger and support women farmers ...    14   \n",
      "4                                      Stop 'N' Swap    31   \n",
      "\n",
      "                                             summary is_priority  category_id  \\\n",
      "0  Building on successful events last summer and ...         NaN          NaN   \n",
      "1             Build a website for an Afghan business         NaN          1.0   \n",
      "2  Please join us and the students from Mott Hall...         NaN          1.0   \n",
      "3  The Oxfam Action Corps is a group of dedicated...         NaN          1.0   \n",
      "4  Stop 'N' Swap reduces NYC's waste by finding n...         NaN          4.0   \n",
      "\n",
      "   amsl  amsl_unit  ...     end_date_date    status  Latitude  Longitude  \\\n",
      "0   NaN        NaN  ...      July 30 2011  approved       NaN        NaN   \n",
      "1   NaN        NaN  ...  February 01 2011  approved       NaN        NaN   \n",
      "2   NaN        NaN  ...   January 29 2011  approved       NaN        NaN   \n",
      "3   NaN        NaN  ...     March 31 2012  approved       NaN        NaN   \n",
      "4   NaN        NaN  ...  February 05 2011  approved       NaN        NaN   \n",
      "\n",
      "   Community Board Community Council  Census Tract  BIN BBL NTA  \n",
      "0              NaN                NaN          NaN  NaN NaN NaN  \n",
      "1              NaN                NaN          NaN  NaN NaN NaN  \n",
      "2              NaN                NaN          NaN  NaN NaN NaN  \n",
      "3              NaN                NaN          NaN  NaN NaN NaN  \n",
      "4              NaN                NaN          NaN  NaN NaN NaN  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of redundant column names to drop\n",
    "# locality, region -> already got postalcode\n",
    "# created_date -> preprocessed and added created_month\n",
    "# vol_requests -> preprocessed and added vol_requests_lognorm\n",
    "# category_desc -> already got one-hot encoded\n",
    "to_drop = [\"locality\", \"region\", \"created_date\", \"vol_requests\", \"category_desc\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of volunteer_subset\n",
    "print(volunteer_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308856f",
   "metadata": {},
   "source": [
    "# Checking for correlated features\n",
    "\n",
    "You'll now return to the `wine` dataset, which consists of continuous, numerical features. Run Pearson's correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Print out the Pearson correlation coefficients for each pair of features in the `wine` dataset.\n",
    "- Drop any columns from `wine` that have a correlation coefficient above 0.75 with at least two other columns ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "792a071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('wine_types.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8bbc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Type   Alcohol  Malic acid       Ash  \\\n",
      "Type                          1.000000 -0.328222    0.437776 -0.049643   \n",
      "Alcohol                      -0.328222  1.000000    0.094397  0.211545   \n",
      "Malic acid                    0.437776  0.094397    1.000000  0.164045   \n",
      "Ash                          -0.049643  0.211545    0.164045  1.000000   \n",
      "Alcalinity of ash             0.517859 -0.310235    0.288500  0.443367   \n",
      "Magnesium                    -0.209179  0.270798   -0.054575  0.286587   \n",
      "Total phenols                -0.719163  0.289101   -0.335167  0.128980   \n",
      "Flavanoids                   -0.847498  0.236815   -0.411007  0.115077   \n",
      "Nonflavanoid phenols          0.489109 -0.155929    0.292977  0.186230   \n",
      "Proanthocyanins              -0.499130  0.136698   -0.220746  0.009652   \n",
      "Color intensity               0.265668  0.546364    0.248985  0.258887   \n",
      "Hue                          -0.617369 -0.071747   -0.561296 -0.074667   \n",
      "OD280/OD315 of diluted wines -0.788230  0.072343   -0.368710  0.003911   \n",
      "Proline                      -0.633717  0.643720   -0.192011  0.223626   \n",
      "\n",
      "                              Alcalinity of ash  Magnesium  Total phenols  \\\n",
      "Type                                   0.517859  -0.209179      -0.719163   \n",
      "Alcohol                               -0.310235   0.270798       0.289101   \n",
      "Malic acid                             0.288500  -0.054575      -0.335167   \n",
      "Ash                                    0.443367   0.286587       0.128980   \n",
      "Alcalinity of ash                      1.000000  -0.083333      -0.321113   \n",
      "Magnesium                             -0.083333   1.000000       0.214401   \n",
      "Total phenols                         -0.321113   0.214401       1.000000   \n",
      "Flavanoids                            -0.351370   0.195784       0.864564   \n",
      "Nonflavanoid phenols                   0.361922  -0.256294      -0.449935   \n",
      "Proanthocyanins                       -0.197327   0.236441       0.612413   \n",
      "Color intensity                        0.018732   0.199950      -0.055136   \n",
      "Hue                                   -0.273955   0.055398       0.433681   \n",
      "OD280/OD315 of diluted wines          -0.276769   0.066004       0.699949   \n",
      "Proline                               -0.440597   0.393351       0.498115   \n",
      "\n",
      "                              Flavanoids  Nonflavanoid phenols  \\\n",
      "Type                           -0.847498              0.489109   \n",
      "Alcohol                         0.236815             -0.155929   \n",
      "Malic acid                     -0.411007              0.292977   \n",
      "Ash                             0.115077              0.186230   \n",
      "Alcalinity of ash              -0.351370              0.361922   \n",
      "Magnesium                       0.195784             -0.256294   \n",
      "Total phenols                   0.864564             -0.449935   \n",
      "Flavanoids                      1.000000             -0.537900   \n",
      "Nonflavanoid phenols           -0.537900              1.000000   \n",
      "Proanthocyanins                 0.652692             -0.365845   \n",
      "Color intensity                -0.172379              0.139057   \n",
      "Hue                             0.543479             -0.262640   \n",
      "OD280/OD315 of diluted wines    0.787194             -0.503270   \n",
      "Proline                         0.494193             -0.311385   \n",
      "\n",
      "                              Proanthocyanins  Color intensity       Hue  \\\n",
      "Type                                -0.499130         0.265668 -0.617369   \n",
      "Alcohol                              0.136698         0.546364 -0.071747   \n",
      "Malic acid                          -0.220746         0.248985 -0.561296   \n",
      "Ash                                  0.009652         0.258887 -0.074667   \n",
      "Alcalinity of ash                   -0.197327         0.018732 -0.273955   \n",
      "Magnesium                            0.236441         0.199950  0.055398   \n",
      "Total phenols                        0.612413        -0.055136  0.433681   \n",
      "Flavanoids                           0.652692        -0.172379  0.543479   \n",
      "Nonflavanoid phenols                -0.365845         0.139057 -0.262640   \n",
      "Proanthocyanins                      1.000000        -0.025250  0.295544   \n",
      "Color intensity                     -0.025250         1.000000 -0.521813   \n",
      "Hue                                  0.295544        -0.521813  1.000000   \n",
      "OD280/OD315 of diluted wines         0.519067        -0.428815  0.565468   \n",
      "Proline                              0.330417         0.316100  0.236183   \n",
      "\n",
      "                              OD280/OD315 of diluted wines   Proline  \n",
      "Type                                             -0.788230 -0.633717  \n",
      "Alcohol                                           0.072343  0.643720  \n",
      "Malic acid                                       -0.368710 -0.192011  \n",
      "Ash                                               0.003911  0.223626  \n",
      "Alcalinity of ash                                -0.276769 -0.440597  \n",
      "Magnesium                                         0.066004  0.393351  \n",
      "Total phenols                                     0.699949  0.498115  \n",
      "Flavanoids                                        0.787194  0.494193  \n",
      "Nonflavanoid phenols                             -0.503270 -0.311385  \n",
      "Proanthocyanins                                   0.519067  0.330417  \n",
      "Color intensity                                  -0.428815  0.316100  \n",
      "Hue                                               0.565468  0.236183  \n",
      "OD280/OD315 of diluted wines                      1.000000  0.312761  \n",
      "Proline                                           0.312761  1.000000  \n",
      "   Type  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
      "0     1    14.23        1.71  2.43               15.6        127   \n",
      "1     1    13.20        1.78  2.14               11.2        100   \n",
      "2     1    13.16        2.36  2.67               18.6        101   \n",
      "3     1    14.37        1.95  2.50               16.8        113   \n",
      "4     1    13.24        2.59  2.87               21.0        118   \n",
      "\n",
      "   Total phenols  Nonflavanoid phenols  Proanthocyanins  Color intensity  \\\n",
      "0           2.80                  0.28             2.29             5.64   \n",
      "1           2.65                  0.26             1.28             4.38   \n",
      "2           2.80                  0.30             2.81             5.68   \n",
      "3           3.85                  0.24             2.18             7.80   \n",
      "4           2.80                  0.39             1.82             4.32   \n",
      "\n",
      "    Hue  OD280/OD315 of diluted wines  Proline  \n",
      "0  1.04                          3.92     1065  \n",
      "1  1.05                          3.40     1050  \n",
      "2  1.03                          3.17     1185  \n",
      "3  0.86                          3.45     1480  \n",
      "4  1.04                          2.93      735  \n"
     ]
    }
   ],
   "source": [
    "# Print out the column correlations of the wine dataset\n",
    "print(wine.corr())\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "wine = wine.drop(['Flavanoids'], axis=1)\n",
    "\n",
    "print(wine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4094d260",
   "metadata": {},
   "source": [
    "# Exploring text vectors, part 1\n",
    "\n",
    "Let's expand on the text vector exploration method we just learned about, using the `volunteer` dataset's `title` tf/idf vectors. In this first part of text vector exploration, we're going to add to that function we learned about in the slides. We'll return a list of numbers with the function. In the next exercise, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our `text_tfidf` vector.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Add parameters called `original_vocab` , for the `tfidf_vec.vocabulary_` , and `top_n` .\n",
    "- Call `pd.Series()` on the zipped dictionary. This will make it easier to operate on.\n",
    "- Use the `.sort_values()` function to sort the series and slice the index up to `top_n` words.\n",
    "- Call the function, setting `original_vocab=tfidf_vec.vocabulary_` , setting `vector_index=8` to grab the 9th row, and setting `top_n=3` , to grab the top 3 weighted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5240ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the volunteer dataset if not already loaded\n",
    "volunteer = pd.read_csv(\"volunteer_opportunities.csv\")\n",
    "\n",
    "# Create TF-IDF vectorizer and fit it on the title column\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "text_tfidf = tfidf_vec.fit_transform(volunteer['title'])\n",
    "\n",
    "# Create vocab as a reverse mapping of the vocabulary\n",
    "vocab = {v: k for k, v in tfidf_vec.vocabulary_.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "938080ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188, 23, 562]\n"
     ]
    }
   ],
   "source": [
    "# Add in the rest of the arguments\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f12798",
   "metadata": {},
   "source": [
    "# Exploring text vectors, part 2\n",
    "\n",
    "Using the `return_weights()` function you wrote in the previous exercise, you're now going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Call `return_weights()` to return the top weighted words for that document.\n",
    "- Call `set()` on the returned `filter_list` to remove duplicated numbers.\n",
    "- Call `words_to_filter` , passing in the following parameters: `vocab` for the `vocab` parameter, `tfidf_vec.vocabulary_` for the `original_vocab` parameter, `text_tfidf` for the `vector` parameter, and `3` to grab the `top_n` 3 weighted words from each document.\n",
    "- Finally, pass that `filtered_words` set into a list to use as a filter for the text vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90576fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# Filter the columns in text_tfidf to only those in filtered_words\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a808a63",
   "metadata": {},
   "source": [
    "# Training Naive Bayes with feature selection\n",
    "\n",
    "You'll now re-run the Naive Bayes text classification model that you ran at the end of Chapter 3 with our selection choices from the previous exercise: the `volunteer` dataset's `title` and `category_desc` columns.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Use `train_test_split()` on the `filtered_text` text vector, the `y` labels (which is the `category_desc` labels), and pass the `y` set to the `stratify` parameter, since we have an uneven class distribution.\n",
    "- Fit the `nb` Naive Bayes model to `X_train` and `y_train` .\n",
    "- Calculate the test set accuracy of `nb` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b94a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a mask for non-null category_desc values\n",
    "mask = volunteer['category_desc'].notna()\n",
    "\n",
    "# Filter both the target variable and the text features using the same mask\n",
    "y = volunteer.loc[mask, 'category_desc']\n",
    "filtered_text_clean = filtered_text[mask]\n",
    "\n",
    "# Create and fit the Naive Bayes model\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eed42d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5548387096774193\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text_clean.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c642d0",
   "metadata": {},
   "source": [
    "# Using PCA\n",
    "\n",
    "In this exercise, you'll apply PCA to the `wine` dataset, to see if you can increase the model's accuracy.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Instantiate a `PCA` object.\n",
    "- Define the features ( `X` ) and labels ( `y` ) from `wine` , using the labels in the `\"Type\"` column.\n",
    "- Apply PCA to `X_train` and `X_test` , ensuring no data leakage, and store the transformed values as `pca_X_train` and `pca_X_test` .\n",
    "- Print out the `.explained_variance_ratio_` attribute of `pca` to check how much variance is explained by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c81dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the wine dataset if not already loaded\n",
    "wine = pd.read_csv('wine_types.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5542ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.97795009e-01 2.02071827e-03 9.88350594e-05 5.66222566e-05\n",
      " 1.26161135e-05 8.93235789e-06 3.13856866e-06 1.57406401e-06\n",
      " 1.15918860e-06 7.49332354e-07 3.70332305e-07 1.94185373e-07\n",
      " 8.08440051e-08]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Define the features and labels from the wine dataset\n",
    "X = wine.drop(\"Type\", axis=1)\n",
    "y = wine[\"Type\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "pca_X_train = pca.fit_transform(X_train)\n",
    "pca_X_test = pca.transform(X_test)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b643f67",
   "metadata": {},
   "source": [
    "# Training a model with PCA\n",
    "\n",
    "Now that you have run PCA on the `wine` dataset, you'll finally train a KNN model using the transformed data.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Fit the `knn` model to the PCA-transformed features, `pca_X_train` , and training labels, `y_train` .\n",
    "- Print the test set accuracy of the `knn` model using `pca_X_test` and `y_test` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ad73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbddb123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "# Fit knn to the training data\n",
    "knn.fit(pca_X_train, y_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "print(knn.score(pca_X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

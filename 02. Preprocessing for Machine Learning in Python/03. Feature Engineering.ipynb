{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8284ed",
   "metadata": {},
   "source": [
    "# Encoding categorical variables - binary\n",
    "\n",
    "Take a look at the `hiking` dataset. There are several columns here that need encoding before they can be modeled, one of which is the `Accessible` column. `Accessible` is a binary feature, so it has two values, `Y` or `N` , which need to be encoded into 1's and 0's. Use scikit-learn's `LabelEncoder` method to perform this transformation.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Store `LabelEncoder()` in a variable named `enc` .\n",
    "- Using the encoder's `.fit_transform()` method, encode the `hiking` dataset's `\"Accessible\"` column. Call the new column `Accessible_enc` .\n",
    "- Compare the two columns side-by-side to see the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "hiking = pd.read_json(\"hiking.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d50ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible', 'Accessible_enc']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668245b",
   "metadata": {},
   "source": [
    "# Encoding categorical variables - one-hot\n",
    "\n",
    "One of the columns in the `volunteer` dataset, `category_desc` , gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use pandas' `pd.get_dummies()` function to do so.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Call `get_dummies()` on the `volunteer[\"category_desc\"]` column to create the encoded columns and assign it to `category_enc` .\n",
    "- Print out the `.head()` of the `category_enc` variable to take a look at the encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd25204",
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer = pd.read_csv(\"volunteer_opportunities.csv\")\n",
    "\n",
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143a1f6",
   "metadata": {},
   "source": [
    "# Aggregating numerical features\n",
    "\n",
    "A good use case for taking an aggregate statistic to create a new feature is when you have many features with similar, related values. Here, you have a DataFrame of running times named `running_times_5k` . For each `name` in the dataset, take the mean of their 5 run times.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Use the `.loc[]` method to select all rows and columns to find the `.mean()` of the each columns.\n",
    "- Print the `.head()` of the DataFrame to see the `mean` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_times_5k = pd.DataFrame({\n",
    "    'name': ['Sue', 'Mark', 'Sean', 'Erin', 'Jenny', 'Russell'],\n",
    "    'run1': [20.1, 16.5, 23.5, 21.7, 25.8, 30.9],\n",
    "    'run2': [18.5, 17.1, 25.1, 21.1, 27.1, 29.6],\n",
    "    'run3': [19.6, 16.9, 25.2, 20.9, 26.1, 31.4],\n",
    "    'run4': [20.3, 17.6, 24.6, 22.1, 26.7, 30.4],\n",
    "    'run5': [18.3, 17.3, 23.9, 22.2, 26.9, 29.9],\n",
    "    'mean': [19.36, 17.08, 24.46, 21.60, 26.52, 30.44]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.loc[:, \"run1\":\"run5\"].mean(axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1825e3",
   "metadata": {},
   "source": [
    "# Extracting datetime components\n",
    "\n",
    "There are several columns in the `volunteer` dataset comprised of datetimes. Let's take a look at the `start_date_date` column and extract just the month to use as a feature for modeling.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Convert the `start_date_date` column into a `pandas` datetime column and store it in a new column called `start_date_converted` .\n",
    "- Retrieve the month component of `start_date_converted` and store it in a new column called `start_date_month` .\n",
    "- Print the `.head()` of just the `start_date_converted` and `start_date_month` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].dt.month\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[[\"start_date_converted\", \"start_date_month\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c2f87",
   "metadata": {},
   "source": [
    "# Extracting string patterns\n",
    "\n",
    "The `Length` column in the `hiking` dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in pandas to apply the extraction to the DataFrame.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Search the text in the `length` argument for numbers and decimals using an appropriate pattern.\n",
    "- Extract the matched pattern and convert it to a float.\n",
    "- Apply the `return_mileage()` function to each row in the `hiking[\"Length\"]` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "hiking = pd.read_json(\"hiking.json\")\n",
    "\n",
    "# remove NaN values from the \"Length\" column\n",
    "hiking = hiking[hiking['Length'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c075e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.search(r'\\d+\\.\\d*', length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(return_mileage)\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b762c",
   "metadata": {},
   "source": [
    "# Vectorizing text\n",
    "\n",
    "You'll now transform the `volunteer` dataset's `title` column into a text vector, which you'll use in a prediction task in the next exercise.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Store the `volunteer[\"title\"]` column in a variable named `title_text` .\n",
    "- Instantiate a `TfidfVectorizer` as `tfidf_vec` .\n",
    "- Transform the text in `title_text` into a tf-idf vector using `tfidf_vec` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3125e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the title text\n",
    "title_text = volunteer['title']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04146f26",
   "metadata": {},
   "source": [
    "# Text classification using tf/idf vectors\n",
    "\n",
    "Now that you've encoded the `volunteer` dataset's `title` column into tf/idf vectors, you'll use those vectors to predict the `category_desc` column.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Split the `text_tfidf` vector and `y` target variable into training and test sets, setting the `stratify` parameter equal to `y` , since the class distribution is uneven. Notice that we have to run the `.toarray()` method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n",
    "- Fit the `X_train` and `y_train` data to the Naive Bayes model, `nb` .\n",
    "- Print out the test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a861a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the title text and remove any NaN values from both title and category_desc\n",
    "volunteer_clean = volunteer[(volunteer['title'].notna()) & (volunteer['category_desc'].notna())]\n",
    "title_text = volunteer_clean['title']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer_clean[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

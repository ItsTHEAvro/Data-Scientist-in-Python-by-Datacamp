{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8284ed",
   "metadata": {},
   "source": [
    "# Encoding categorical variables - binary\n",
    "\n",
    "Take a look at the `hiking` dataset. There are several columns here that need encoding before they can be modeled, one of which is the `Accessible` column. `Accessible` is a binary feature, so it has two values, `Y` or `N` , which need to be encoded into 1's and 0's. Use scikit-learn's `LabelEncoder` method to perform this transformation.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Store `LabelEncoder()` in a variable named `enc` .\n",
    "- Using the encoder's `.fit_transform()` method, encode the `hiking` dataset's `\"Accessible\"` column. Call the new column `Accessible_enc` .\n",
    "- Compare the two columns side-by-side to see the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434f55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "hiking = pd.read_json(\"hiking.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d50ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accessible  Accessible_enc\n",
      "0          Y               1\n",
      "1          N               0\n",
      "2          N               0\n",
      "3          N               0\n",
      "4          N               0\n"
     ]
    }
   ],
   "source": [
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible', 'Accessible_enc']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668245b",
   "metadata": {},
   "source": [
    "# Encoding categorical variables - one-hot\n",
    "\n",
    "One of the columns in the `volunteer` dataset, `category_desc` , gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use pandas' `pd.get_dummies()` function to do so.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Call `get_dummies()` on the `volunteer[\"category_desc\"]` column to create the encoded columns and assign it to `category_enc` .\n",
    "- Print out the `.head()` of the `category_enc` variable to take a look at the encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd25204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Education  Emergency Preparedness  Environment  Health  \\\n",
      "0      False                   False        False   False   \n",
      "1      False                   False        False   False   \n",
      "2      False                   False        False   False   \n",
      "3      False                   False        False   False   \n",
      "4      False                   False         True   False   \n",
      "\n",
      "   Helping Neighbors in Need  Strengthening Communities  \n",
      "0                      False                      False  \n",
      "1                      False                       True  \n",
      "2                      False                       True  \n",
      "3                      False                       True  \n",
      "4                      False                      False  \n"
     ]
    }
   ],
   "source": [
    "volunteer = pd.read_csv(\"volunteer_opportunities.csv\")\n",
    "\n",
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143a1f6",
   "metadata": {},
   "source": [
    "# Aggregating numerical features\n",
    "\n",
    "A good use case for taking an aggregate statistic to create a new feature is when you have many features with similar, related values. Here, you have a DataFrame of running times named `running_times_5k` . For each `name` in the dataset, take the mean of their 5 run times.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Use the `.loc[]` method to select all rows and columns to find the `.mean()` of the each columns.\n",
    "- Print the `.head()` of the DataFrame to see the `mean` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e9a2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_times_5k = pd.DataFrame({\n",
    "    'name': ['Sue', 'Mark', 'Sean', 'Erin', 'Jenny', 'Russell'],\n",
    "    'run1': [20.1, 16.5, 23.5, 21.7, 25.8, 30.9],\n",
    "    'run2': [18.5, 17.1, 25.1, 21.1, 27.1, 29.6],\n",
    "    'run3': [19.6, 16.9, 25.2, 20.9, 26.1, 31.4],\n",
    "    'run4': [20.3, 17.6, 24.6, 22.1, 26.7, 30.4],\n",
    "    'run5': [18.3, 17.3, 23.9, 22.2, 26.9, 29.9],\n",
    "    'mean': [19.36, 17.08, 24.46, 21.60, 26.52, 30.44]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c65792d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name  run1  run2  run3  run4  run5   mean\n",
      "0    Sue  20.1  18.5  19.6  20.3  18.3  19.36\n",
      "1   Mark  16.5  17.1  16.9  17.6  17.3  17.08\n",
      "2   Sean  23.5  25.1  25.2  24.6  23.9  24.46\n",
      "3   Erin  21.7  21.1  20.9  22.1  22.2  21.60\n",
      "4  Jenny  25.8  27.1  26.1  26.7  26.9  26.52\n"
     ]
    }
   ],
   "source": [
    "# Use .loc to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.loc[:, \"run1\":\"run5\"].mean(axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1825e3",
   "metadata": {},
   "source": [
    "# Extracting datetime components\n",
    "\n",
    "There are several columns in the `volunteer` dataset comprised of datetimes. Let's take a look at the `start_date_date` column and extract just the month to use as a feature for modeling.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Convert the `start_date_date` column into a `pandas` datetime column and store it in a new column called `start_date_converted` .\n",
    "- Retrieve the month component of `start_date_converted` and store it in a new column called `start_date_month` .\n",
    "- Print the `.head()` of just the `start_date_converted` and `start_date_month` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a7cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  start_date_converted  start_date_month\n",
      "0           2011-07-30                 7\n",
      "1           2011-02-01                 2\n",
      "2           2011-01-29                 1\n",
      "3           2011-02-14                 2\n",
      "4           2011-02-05                 2\n"
     ]
    }
   ],
   "source": [
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].dt.month\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[[\"start_date_converted\", \"start_date_month\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c2f87",
   "metadata": {},
   "source": [
    "# Extracting string patterns\n",
    "\n",
    "The `Length` column in the `hiking` dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in pandas to apply the extraction to the DataFrame.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Search the text in the `length` argument for numbers and decimals using an appropriate pattern.\n",
    "- Extract the matched pattern and convert it to a float.\n",
    "- Apply the `return_mileage()` function to each row in the `hiking[\"Length\"]` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d64547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "hiking = pd.read_json(\"hiking.json\")\n",
    "\n",
    "# remove NaN values from the \"Length\" column\n",
    "hiking = hiking[hiking['Length'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c075e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Length  Length_num\n",
      "0   0.8 miles        0.80\n",
      "1    1.0 mile        1.00\n",
      "2  0.75 miles        0.75\n",
      "3   0.5 miles        0.50\n",
      "4   0.5 miles        0.50\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.search(r'\\d+\\.\\d*', length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(return_mileage)\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b762c",
   "metadata": {},
   "source": [
    "# Vectorizing text\n",
    "\n",
    "You'll now transform the `volunteer` dataset's `title` column into a text vector, which you'll use in a prediction task in the next exercise.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Store the `volunteer[\"title\"]` column in a variable named `title_text` .\n",
    "- Instantiate a `TfidfVectorizer` as `tfidf_vec` .\n",
    "- Transform the text in `title_text` into a tf-idf vector using `tfidf_vec` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b8cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3125e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the title text\n",
    "title_text = volunteer['title']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04146f26",
   "metadata": {},
   "source": [
    "# Text classification using tf/idf vectors\n",
    "\n",
    "Now that you've encoded the `volunteer` dataset's `title` column into tf/idf vectors, you'll use those vectors to predict the `category_desc` column.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Split the `text_tfidf` vector and `y` target variable into training and test sets, setting the `stratify` parameter equal to `y` , since the class distribution is uneven. Notice that we have to run the `.toarray()` method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n",
    "- Fit the `X_train` and `y_train` data to the Naive Bayes model, `nb` .\n",
    "- Print out the test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a861a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5548387096774193\n"
     ]
    }
   ],
   "source": [
    "# Take the title text and remove any NaN values from both title and category_desc\n",
    "volunteer_clean = volunteer[(volunteer['title'].notna()) & (volunteer['category_desc'].notna())]\n",
    "title_text = volunteer_clean['title']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer_clean[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
